{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(nn.Module):\n",
    "    \"\"\"Parametrized Policy Network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_space1 = 16  # Nothing special with 16, feel free to change\n",
    "        hidden_space2 = 32  # Nothing special with 32, feel free to change\n",
    "\n",
    "        # Shared Network\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_space_dims, hidden_space1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_space1, hidden_space2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Policy Mean specific Linear Layer\n",
    "        self.policy_mean_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "        # Policy Std Dev specific Linear Layer\n",
    "        self.policy_stddev_net = nn.Sequential(\n",
    "            nn.Linear(hidden_space2, action_space_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
    "         of a normal distribution from which an action is sampled from.\n",
    "\n",
    "        Args:\n",
    "            x: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action_means: predicted mean of the normal distribution\n",
    "            action_stddevs: predicted standard deviation of the normal distribution\n",
    "        \"\"\"\n",
    "        shared_features = self.shared_net(x.float())\n",
    "\n",
    "        action_means = self.policy_mean_net(shared_features)\n",
    "        action_stddevs = torch.log(\n",
    "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
    "        )\n",
    "\n",
    "        return action_means, action_stddevs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"REINFORCE algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
    "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm [1]\n",
    "        to solve the task at hand (Inverted Pendulum v4).\n",
    "\n",
    "        Args:\n",
    "            obs_space_dims: Dimension of the observation space\n",
    "            action_space_dims: Dimension of the action space\n",
    "        \"\"\"\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 1e-4  # Learning rate for policy optimization\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.eps = 1e-6  # small number for mathematical stability\n",
    "\n",
    "        self.probs = []  # Stores probability values of the sampled action\n",
    "        self.rewards = []  # Stores the corresponding rewards\n",
    "\n",
    "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
    "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def sample_action(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Returns an action, conditioned on the policy and observation.\n",
    "\n",
    "        Args:\n",
    "            state: Observation from the environment\n",
    "\n",
    "        Returns:\n",
    "            action: Action to be performed\n",
    "        \"\"\"\n",
    "        state = torch.tensor(np.array([state]))\n",
    "        action_means, action_stddevs = self.net(state)\n",
    "\n",
    "        # create a normal distribution from the predicted\n",
    "        #   mean and standard deviation and sample an action\n",
    "        distrib = Normal(action_means[0] + self.eps, action_stddevs[0] + self.eps)\n",
    "        action = distrib.sample()\n",
    "        prob = distrib.log_prob(action)\n",
    "\n",
    "        action = action.numpy()\n",
    "\n",
    "        self.probs.append(prob)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Updates the policy network's weights.\"\"\"\n",
    "        running_g = 0\n",
    "        gs = []\n",
    "\n",
    "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
    "        for R in self.rewards[::-1]:\n",
    "            running_g = R + self.gamma * running_g\n",
    "            gs.insert(0, running_g)\n",
    "\n",
    "        deltas = torch.tensor(gs)\n",
    "\n",
    "        loss = 0\n",
    "        # minimize -1 * prob * reward obtained\n",
    "        for log_prob, delta in zip(self.probs, deltas):\n",
    "            loss += log_prob.mean() * delta * (-1)\n",
    "\n",
    "        # Update the policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Empty / zero out all episode-centric/related variables\n",
    "        self.probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/5000 [00:00<00:50, 98.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1006/5000 [00:14<01:43, 38.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 Average Reward: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2002/5000 [01:09<03:48, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000 Average Reward: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3002/5000 [02:32<03:01, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3000 Average Reward: 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4002/5000 [04:19<02:21,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4000 Average Reward: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:33<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/5000 [00:00<00:41, 119.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1014/5000 [00:11<00:56, 70.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 Average Reward: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2006/5000 [00:37<02:07, 23.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000 Average Reward: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3003/5000 [01:47<02:45, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3000 Average Reward: 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [03:33<02:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4000 Average Reward: 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:49<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/5000 [00:00<00:50, 99.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Average Reward: 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1010/5000 [00:09<00:42, 92.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000 Average Reward: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2007/5000 [00:26<01:18, 38.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000 Average Reward: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3002/5000 [01:38<05:34,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3000 Average Reward: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [07:13<11:29,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4000 Average Reward: 867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 4190/5000 [09:12<01:46,  7.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample_action(obs)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# These represent the next observation, the reward from the step,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# if the episode is terminated, if the episode is truncated and\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# additional info from the step\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m agent\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# End the episode when either truncated or terminated is true\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#  - truncated: The episode duration reaches max number of timesteps\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#  - terminated: Any of the state space values is no longer finite.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/wrappers/record_episode_statistics.py:94\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     (\n\u001b[1;32m     89\u001b[0m         observations,\n\u001b[1;32m     90\u001b[0m         rewards,\n\u001b[1;32m     91\u001b[0m         terminations,\n\u001b[1;32m     92\u001b[0m         truncations,\n\u001b[1;32m     93\u001b[0m         infos,\n\u001b[0;32m---> 94\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m     96\u001b[0m         infos, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m     97\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`info` dtype is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(infos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/envs/mujoco/inverted_pendulum_v4.py:119\u001b[0m, in \u001b[0;36mInvertedPendulumEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[1;32m    118\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     ob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[1;32m    121\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(ob)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;129;01mor\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mabs(ob[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.2\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:177\u001b[0m, in \u001b[0;36mBaseMujocoEnv.do_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension mismatch. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnu,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_mujoco_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/intobl/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:401\u001b[0m, in \u001b[0;36mMujocoEnv._step_mujoco_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_mujoco_simulation\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctrl, n_frames):\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mctrl[:] \u001b[38;5;241m=\u001b[39m ctrl\n\u001b[0;32m--> 401\u001b[0m     \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# As of MuJoCo 2.0, force-related quantities like cacc are not computed\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# unless there's a force sensor in the model.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# See https://github.com/openai/gym/issues/1541\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     mujoco\u001b[38;5;241m.\u001b[39mmj_rnePostConstraint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = gym.make(\"InvertedPendulum-v4\")\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
    "\n",
    "total_num_episodes = int(5e3)  # Total number of episodes\n",
    "# Observation-space of InvertedPendulum-v4 (4)\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "# Action-space of InvertedPendulum-v4 (1)\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in [1, 2, 3, 5, 8]:  # Fibonacci seeds\n",
    "    # set seed\n",
    "    print(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Reinitialize agent every seed\n",
    "    agent = REINFORCE(obs_space_dims, action_space_dims)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    for episode in tqdm(range(total_num_episodes)):\n",
    "        # gymnasium v26 requires users to set seed while resetting the environment\n",
    "        obs, info = wrapped_env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.sample_action(obs)\n",
    "\n",
    "            # Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\n",
    "            # These represent the next observation, the reward from the step,\n",
    "            # if the episode is terminated, if the episode is truncated and\n",
    "            # additional info from the step\n",
    "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "\n",
    "            # End the episode when either truncated or terminated is true\n",
    "            #  - truncated: The episode duration reaches max number of timesteps\n",
    "            #  - terminated: Any of the state space values is no longer finite.\n",
    "            done = terminated or truncated\n",
    "\n",
    "        reward_over_episodes.append(wrapped_env.return_queue[-1])\n",
    "        agent.update()\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "    rewards_over_seeds.append(reward_over_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intobl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
